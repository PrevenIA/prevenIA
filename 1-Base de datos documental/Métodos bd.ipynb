{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e992fe-370a-434f-875a-00bd1de9cc2d",
   "metadata": {},
   "source": [
    "En este cuardenos vamos a crear unos métodos para crear la base de datos o actualizarla si ha habido cambios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "052beea0-6372-4d79-ac10-3a5b6b6945ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texto_correcto(texto):\n",
    "    num = len([x for x in texto if not x.isalnum() and x not in ['.', ',', ';', ':', ' ', '?', '¿', '¡', '!', '(', ')']])\n",
    "    return num < 10, num\n",
    "\n",
    "def get_chunks(texto, tam_chunks = 1000):\n",
    "    ultimo_punto = 0\n",
    "    inicio_chunk = 0\n",
    "    posicion_texto = 0\n",
    "    contador_chunk = 0\n",
    "    aux = []\n",
    "    resultado = []\n",
    "    while posicion_texto < len(texto):\n",
    "        if contador_chunk == 0:\n",
    "            chunk = ''\n",
    "        chunk += texto[posicion_texto]\n",
    "        if texto[posicion_texto] == '.':\n",
    "            ultimo_punto = posicion_texto\n",
    "        posicion_texto += 1\n",
    "        contador_chunk += 1\n",
    "        if contador_chunk == tam_chunks:\n",
    "            aux.append(chunk)\n",
    "            contador_chunk = 0\n",
    "            if posicion_texto != ultimo_punto:\n",
    "                if inicio_chunk < ultimo_punto:\n",
    "                    posicion_texto = ultimo_punto+2                    \n",
    "            inicio_chunk = posicion_texto\n",
    "    if contador_chunk != 0: \n",
    "        aux.append(chunk)\n",
    "    \n",
    "    for chunk in aux:\n",
    "        alnum = sum(c.isalnum() for c in chunk)\n",
    "        letra = sum(c.isalpha() for c in chunk)\n",
    "        todos = len(chunk)\n",
    "        if  alnum/todos > 0.78 and len(chunk) > tam_chunks*0.25 and texto_correcto(chunk)[0] and letra/alnum > 0.9:\n",
    "            resultado.append(chunk)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3adf0b23-5abd-49e3-bbbe-55739271fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "from io import StringIO\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "def eliminarDivisionesSilabicas(s):\n",
    "    palabrasObejtivo = re.findall(r\"\\w+-\\n\\w+\",s)\n",
    "    sOut = s\n",
    "    for m in palabrasObejtivo:\n",
    "        new = m.replace(\"-\\n\",\"\")\n",
    "        sOut = sOut.replace(m,new)\n",
    "    return sOut\n",
    "\n",
    "def corregir_indice(s):\n",
    "    return re.sub(r\"\\.\\.\\.\\.+\", '.', s)\n",
    "\n",
    "def convertirPDF(nombreFichero):\n",
    "    numPag = set()\n",
    "    output = io.StringIO()\n",
    "    manager = PDFResourceManager()\n",
    "    converter = TextConverter(manager, output, laparams=LAParams())\n",
    "    interpreter = PDFPageInterpreter(manager, converter)\n",
    "\n",
    "    infile = open(nombreFichero, 'rb')\n",
    "    for page in PDFPage.get_pages(infile, numPag):\n",
    "        interpreter.process_page(page)\n",
    "    infile.close()\n",
    "    converter.close()\n",
    "    text = output.getvalue()\n",
    "    output.close\n",
    "    text = eliminarDivisionesSilabicas(corregir_indice(text)).replace(\"\\n\", \" \").replace(\"\\x0c\", \" \")\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e7f4c7f3-1eec-42b4-8ba1-743344e850cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "def crearBaseDatos(ruta_documentos = \"Documentos\"):\n",
    "    contador = 1\n",
    "    chunks = []\n",
    "    checksums = []\n",
    "    documentos = glob.glob(\"Documentos/*\")\n",
    "    m = len(documentos)\n",
    "    for file in documentos:\n",
    "        fichero = file.split(\"/\")[1]\n",
    "        checksums += [(hashlib.md5(open(file,'rb').read()).hexdigest(), fichero)]\n",
    "        # Extraemos el texto del PDF\n",
    "        texto = convertirPDF(file)\n",
    "        chunks += [(elem, fichero) for elem in get_chunks(texto)]\n",
    "        \n",
    "        print(f'Documento {contador} de {m}', end=\"\\r\")\n",
    "        contador += 1\n",
    "\n",
    "    pd.DataFrame(chunks, columns=['Contexto', 'Documento']).to_csv(\"Contextos.csv\", sep=',', index=False)\n",
    "    pd.DataFrame(checksums, columns=['Checksum', 'Documento']).to_csv(\"Checksums.csv\", sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8f7cc14-586c-4cbe-ab51-bd390f03decd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def actualizarBaseDatos(ruta_documentos = \"Documentos\"):\n",
    "    ficheros = glob.glob(\"Documentos/*\")\n",
    "    documentos = [elem.split(\"/\")[1] for elem in ficheros]\n",
    "    df_contextos = pd.read_csv('Contextos.csv')\n",
    "    df_checksums = pd.read_csv('Checksums.csv')\n",
    "    \n",
    "    documentos_persistidos = df_checksums['Documento'].tolist()\n",
    "    documentos_checksums = df_checksums['Checksum'].tolist()\n",
    "    # Primero comprobamos si hay ficheros nuevos\n",
    "    documentos_nuevos = list(set(documentos) - set(documentos_persistidos))\n",
    "    checksums, chunks = [], []\n",
    "    for nuevo in documentos_nuevos:\n",
    "        checksum = hashlib.md5(open(f'Documentos/{nuevo}','rb').read()).hexdigest()\n",
    "        if checksum in documentos_checksums:\n",
    "            nombre_viejo = df_checksums[df_checksums[\"Checksum\"] == checksum].Documento.values[0]\n",
    "            df_checksums.loc[df_checksums.Checksum == checksum, 'Documento'] = nuevo\n",
    "            df_contextos.loc[df_contextos.Documento == nombre_viejo, 'Documento'] = nuevo\n",
    "        else:\n",
    "            checksums += [(hashlib.md5(open(f'Documentos/{nuevo}','rb').read()).hexdigest(), nuevo)]\n",
    "            texto = convertirPDF(f'Documentos/{nuevo}')\n",
    "            chunks += [(elem, nuevo) for elem in get_chunks(texto)]\n",
    "    df_contextos = pd.concat([df_contextos, pd.DataFrame(chunks, columns=['Contexto', 'Documento'])])\n",
    "    df_checksums = pd.concat([df_checksums, pd.DataFrame(checksums, columns=['Checksum', 'Documento'])])\n",
    "    \n",
    "    # Segundo eliminamos los que ya no estén entre los documentos\n",
    "    documentos_eliminar = list(set(documentos_persistidos) - set(documentos))\n",
    "    for eliminar in documentos_eliminar:\n",
    "        df_contextos.drop(df_contextos.loc[df_contextos['Documento']==eliminar].index, inplace=True)\n",
    "        df_checksums.drop(df_checksums.loc[df_checksums['Documento']==eliminar].index, inplace=True)\n",
    "    \n",
    "    # Persistimos los cambios\n",
    "    df_contextos.sort_values('Documento').to_csv(\"Contextos.csv\", sep=',', index=False)\n",
    "    df_checksums.sort_values('Documento').to_csv(\"Checksums.csv\", sep=',', index=False)\n",
    "    print(\"Operación completada\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:fastai]",
   "language": "python",
   "name": "conda-env-fastai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
